# AssayBio 日志聚合配置
# Fluentd 企业级日志收集和处理

# ====================
# 系统配置
# ====================
<system>
  log_level info
  suppress_repeated_stacktrace true
  emit_error_log_interval 30
  suppress_config_dump
  without_source
  workers 2
</system>

# ====================
# 输入源配置
# ====================

# Nginx访问日志
<source>
  @type tail
  @id nginx_access_log
  path /var/log/nginx/access.log
  pos_file /var/log/fluentd/nginx-access.log.pos
  tag nginx.access
  format nginx
  types size:integer,reqtime:float,upstreamtime:float
  keep_time_key true
  refresh_interval 5
</source>

# Nginx错误日志
<source>
  @type tail
  @id nginx_error_log
  path /var/log/nginx/error.log
  pos_file /var/log/fluentd/nginx-error.log.pos
  tag nginx.error
  format multiline
  format_firstline /^\d{4}\/\d{2}\/\d{2} \d{2}:\d{2}:\d{2} \[/
  format1 /^(?<time>\d{4}\/\d{2}\/\d{2} \d{2}:\d{2}:\d{2}) \[(?<log_level>\w+)\] (?<message>.*)/
  time_format %Y/%m/%d %H:%M:%S
  keep_time_key true
  refresh_interval 5
</source>

# JSON格式的分析日志
<source>
  @type tail
  @id nginx_analytics_log
  path /var/log/nginx/analytics.log
  pos_file /var/log/fluentd/nginx-analytics.log.pos
  tag nginx.analytics
  format json
  keep_time_key true
  refresh_interval 5
</source>

# 容器日志
<source>
  @type forward
  @id docker_logs
  port 24224
  bind 0.0.0.0
</source>

# 系统日志
<source>
  @type systemd
  @id systemd_logs
  tag systemd
  path /var/log/journal
  matches [{ "_SYSTEMD_UNIT": "nginx.service" }]
  read_from_head false
  <storage>
    @type local
    persistent true
    path /var/log/fluentd/systemd.pos
  </storage>
  <entry>
    fields_strip_underscores true
    field_map {"MESSAGE": "message", "_SYSTEMD_UNIT": "unit"}
  </entry>
</source>

# ====================
# 过滤和处理
# ====================

# 解析Nginx访问日志中的User-Agent
<filter nginx.access>
  @type parser
  @id parse_user_agent
  key_name agent
  reserve_data true
  inject_key_prefix ua_
  suppress_parse_error_log false
  <parse>
    @type user_agent
  </parse>
</filter>

# 地理位置信息解析
<filter nginx.access>
  @type geoip
  @id geoip_lookup
  geoip_lookup_keys remote
  <record>
    country_code ${country_code["remote"]}
    country_name ${country_name["remote"]}
    city_name ${city_name["remote"]}
    latitude ${latitude["remote"]}
    longitude ${longitude["remote"]}
  </record>
  remove_tag_prefix nginx.
  add_tag_prefix geoip.nginx.
</filter>

# 添加环境标识
<filter nginx.**>
  @type record_transformer
  @id add_environment
  <record>
    environment production
    service assaybio-website
    hostname "#{Socket.gethostname}"
  </record>
</filter>

# 错误日志告警过滤
<filter nginx.error>
  @type grep
  @id critical_errors
  <regexp>
    key log_level
    pattern ^(error|crit|alert|emerg)$
  </regexp>
  add_tag_prefix alert.
</filter>

# 状态码分类
<filter nginx.access>
  @type record_transformer
  @id status_classification
  <record>
    status_class ${
      case status.to_i
      when 200..299
        'success'
      when 300..399
        'redirect'
      when 400..499
        'client_error'
      when 500..599
        'server_error'
      else
        'unknown'
      end
    }
  </record>
</filter>

# 响应时间分类
<filter nginx.access>
  @type record_transformer
  @id response_time_classification
  <record>
    response_time_class ${
      rt = request_time.to_f
      case
      when rt < 0.1
        'fast'
      when rt < 0.5
        'normal'
      when rt < 2.0
        'slow'
      else
        'very_slow'
      end
    }
  </record>
</filter>

# ====================
# 路由配置
# ====================

# 高频访问日志采样
<match nginx.access>
  @type rewrite_tag_filter
  @id access_log_sampling
  <rule>
    key status
    pattern ^[45]\d\d$
    tag error.nginx.access
  </rule>
  <rule>
    key path
    pattern ^/(health|ping|status)$
    tag health.nginx.access
  </rule>
  <rule>
    key path
    pattern .*
    tag normal.nginx.access
  </rule>
</match>

# ====================
# 输出配置
# ====================

# 错误日志立即发送到告警系统
<match error.nginx.**>
  @type http
  @id error_alerts
  endpoint_url http://alertmanager:9093/api/v1/alerts
  http_method post
  headers {"Content-Type":"application/json"}
  <format>
    @type json
  </format>
  <buffer>
    @type memory
    flush_interval 1s
    chunk_limit_size 1m
  </buffer>
</match>

# 健康检查日志 - 采样存储
<match health.nginx.access>
  @type copy
  <store>
    @type file
    @id health_logs_file
    path /var/log/fluentd/health
    append true
    time_slice_format %Y%m%d
    time_slice_wait 10m
    time_format %Y%m%dT%H%M%S%z
    <buffer>
      @type file
      path /var/log/fluentd/buffer/health
      flush_interval 60s
      chunk_limit_size 8m
    </buffer>
    <format>
      @type json
    </format>
  </store>
</match>

# 正常访问日志 - 结构化存储
<match normal.nginx.access>
  @type copy
  <store>
    # 本地文件存储
    @type file
    @id access_logs_file
    path /var/log/fluentd/access
    append true
    time_slice_format %Y%m%d/%H
    time_slice_wait 10m
    compress gzip
    <buffer>
      @type file
      path /var/log/fluentd/buffer/access
      flush_interval 300s
      chunk_limit_size 64m
      total_limit_size 2g
      overflow_action drop_oldest_chunk
    </buffer>
    <format>
      @type json
    </format>
  </store>
  
  # 发送到分析系统（如果有）
  # <store>
  #   @type elasticsearch
  #   @id elasticsearch_access
  #   host elasticsearch
  #   port 9200
  #   index_name assaybio-access-logs
  #   type_name access_log
  #   <buffer>
  #     @type memory
  #     flush_interval 30s
  #     chunk_limit_size 32m
  #   </buffer>
  # </store>
</match>

# Nginx错误日志
<match nginx.error>
  @type copy
  <store>
    @type file
    @id error_logs_file
    path /var/log/fluentd/error
    append true
    time_slice_format %Y%m%d
    time_slice_wait 10m
    <buffer>
      @type file
      path /var/log/fluentd/buffer/error
      flush_interval 60s
      chunk_limit_size 8m
    </buffer>
    <format>
      @type json
    </format>
  </store>
</match>

# 分析日志
<match nginx.analytics>
  @type copy
  <store>
    @type file
    @id analytics_logs_file
    path /var/log/fluentd/analytics
    append true
    time_slice_format %Y%m%d
    time_slice_wait 10m
    <buffer>
      @type file
      path /var/log/fluentd/buffer/analytics
      flush_interval 300s
      chunk_limit_size 32m
    </buffer>
    <format>
      @type json
    </format>
  </store>
  
  # 实时分析处理
  # <store>
  #   @type kafka2
  #   @id analytics_kafka
  #   brokers kafka:9092
  #   topic_key service
  #   default_topic assaybio-analytics
  #   <format>
  #     @type json
  #   </format>
  # </store>
</match>

# 系统日志
<match systemd>
  @type file
  @id systemd_logs_file
  path /var/log/fluentd/systemd
  append true
  time_slice_format %Y%m%d
  time_slice_wait 10m
  <buffer>
    @type file
    path /var/log/fluentd/buffer/systemd
    flush_interval 300s
    chunk_limit_size 16m
  </buffer>
  <format>
    @type json
  </format>
</match>

# 告警日志
<match alert.**>
  @type copy
  <store>
    @type file
    @id alert_logs_file
    path /var/log/fluentd/alerts
    append true
    time_slice_format %Y%m%d
    time_slice_wait 1m
    <buffer>
      @type file
      path /var/log/fluentd/buffer/alerts
      flush_interval 10s
      chunk_limit_size 1m
    </buffer>
    <format>
      @type json
    </format>
  </store>
  
  # 发送到Slack或邮件
  # <store>
  #   @type slack
  #   @id slack_alerts
  #   webhook_url "#{ENV['SLACK_WEBHOOK']}"
  #   channel alerts
  #   username FluentdBot
  #   icon_emoji :warning:
  #   title Alert from AssayBio
  #   <buffer>
  #     @type memory
  #     flush_interval 1s
  #   </buffer>
  # </store>
</match>

# 默认输出 - 捕获未匹配的日志
<match **>
  @type file
  @id default_output
  path /var/log/fluentd/unmatched
  append true
  time_slice_format %Y%m%d
  time_slice_wait 10m
  <buffer>
    @type file
    path /var/log/fluentd/buffer/unmatched
    flush_interval 300s
    chunk_limit_size 8m
  </buffer>
  <format>
    @type json
  </format>
</match>