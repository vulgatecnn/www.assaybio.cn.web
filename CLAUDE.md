# CLAUDE.md - AssayBio111 ç½‘ç«™çˆ¬è™«é¡¹ç›®å¼€å‘æŒ‡å—

## é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®æ˜¯AssayBioä¼ä¸šçº§ç½‘ç«™çˆ¬è™«å’Œå†…å®¹è¿ç§»é¡¹ç›®ï¼Œä¸“æ³¨äºä»legacyç½‘ç«™æŠ“å–å†…å®¹å¹¶æ„å»ºç°ä»£åŒ–çš„Vue3ç½‘ç«™ã€‚

### é¡¹ç›®æ¶æ„
- **ç½‘ç«™åº”ç”¨**: Vue3 + TypeScript + Vite (apps/website)
- **å†…å®¹æŠ“å–å·¥å…·**: Python + requests + BeautifulSoup (tools/content-scraper)
- **éƒ¨ç½²**: Docker + Nginx
- **åŒ…ç®¡ç†**: pnpm workspace (å‰ç«¯) + pip (Pythonå·¥å…·)

## ç½‘é¡µçˆ¬è™«å·¥å…·æ ‡å‡†

### ğŸš¨ é‡è¦ï¼šPythonçˆ¬è™«æŠ€æœ¯æ ˆ

**æœ¬é¡¹ç›®ä½¿ç”¨PythonæŠ€æœ¯æ ˆè¿›è¡Œç½‘é¡µçˆ¬è™«å¼€å‘ï¼Œä¸¥ç¦ä½¿ç”¨æµè§ˆå™¨è‡ªåŠ¨åŒ–å·¥å…·ã€‚**

#### æŠ€æœ¯é€‰å‹
1. **Python + requests + BeautifulSoup** ä½œä¸ºä¸»è¦çˆ¬è™«æŠ€æœ¯æ ˆ
2. **ä¸¥ç¦ä½¿ç”¨æµè§ˆå™¨è‡ªåŠ¨åŒ–å·¥å…·**ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š
   - Playwright
   - Selenium WebDriver
   - Puppeteer
   - Chrome DevTools Protocol (CDP)
   - ä»»ä½•å…¶ä»–æµè§ˆå™¨è‡ªåŠ¨åŒ–åº“

3. **æ¨èçš„Pythonçˆ¬è™«åº“**ï¼š
   - `requests` - HTTPè¯·æ±‚
   - `BeautifulSoup` - HTMLè§£æ
   - `lxml` - XML/HTMLè§£æå™¨
   - `urllib` - URLå¤„ç†
   - `json` - JSONæ•°æ®å¤„ç†
   - `time` - è¯·æ±‚å»¶æ—¶æ§åˆ¶
   - `concurrent.futures` - å¹¶å‘æ§åˆ¶

### Pythonçˆ¬è™«æœ€ä½³å®è·µ

#### åŸºç¡€é…ç½®
```python
import requests
from bs4 import BeautifulSoup
import time
from urllib.parse import urljoin, urlparse
from concurrent.futures import ThreadPoolExecutor
import json

# æ ‡å‡†è¯·æ±‚é…ç½®
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Accept-Encoding': 'gzip, deflate',
    'Connection': 'keep-alive'
}

session = requests.Session()
session.headers.update(headers)
```

#### ç½‘é¡µæŠ“å–æ¨¡å¼
```python
def scrape_page(url: str) -> dict:
    """åŸºç¡€é¡µé¢æŠ“å–"""
    try:
        response = session.get(url, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # æå–æ•°æ®
        data = {
            'url': url,
            'title': soup.find('title').text if soup.find('title') else '',
            'content': soup.get_text(),
            'links': [urljoin(url, a.get('href')) for a in soup.find_all('a', href=True)]
        }
        
        return data
    except requests.RequestException as e:
        print(f"Error scraping {url}: {e}")
        return None
```

#### é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
```python
def scrape_with_retry(url: str, max_retries: int = 3, delay: float = 2.0) -> dict:
    """å¸¦é‡è¯•æœºåˆ¶çš„é¡µé¢æŠ“å–"""
    for attempt in range(max_retries):
        try:
            response = session.get(url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            return extract_page_data(soup, url)
            
        except requests.RequestException as e:
            print(f"Attempt {attempt + 1} failed for {url}: {e}")
            if attempt < max_retries - 1:
                time.sleep(delay * (2 ** attempt))  # æŒ‡æ•°é€€é¿
            else:
                raise e
```

#### å¹¶å‘æ§åˆ¶å’Œæ€§èƒ½ä¼˜åŒ–
```python
def scrape_urls_concurrent(urls: list, max_workers: int = 5, delay: float = 1.0):
    """å¹¶å‘æŠ“å–å¤šä¸ªURL"""
    results = []
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = []
        
        for url in urls:
            future = executor.submit(scrape_with_delay, url, delay)
            futures.append(future)
        
        for future in futures:
            try:
                result = future.result()
                if result:
                    results.append(result)
            except Exception as e:
                print(f"Error in concurrent scraping: {e}")
    
    return results

def scrape_with_delay(url: str, delay: float = 1.0):
    """å¸¦å»¶æ—¶çš„æŠ“å–"""
    time.sleep(delay)
    return scrape_page(url)
```

### é¡¹ç›®ç‰¹å®šè¦æ±‚

#### AssayBioç½‘ç«™æŠ“å–è§„èŒƒ
1. **ä¸¥æ ¼éµå¾ªrobots.txt**
2. **è¯·æ±‚é—´éš”**: æ¯ä¸ªè¯·æ±‚é—´éš”è‡³å°‘2ç§’
3. **å¹¶å‘é™åˆ¶**: æœ€å¤šåŒæ—¶3ä¸ªå¹¶å‘è¯·æ±‚
4. **è¶…æ—¶è®¾ç½®**: HTTPè¯·æ±‚è¶…æ—¶30ç§’
5. **ç”¨æˆ·ä»£ç†**: ä½¿ç”¨æ ‡å‡†Chromeç”¨æˆ·ä»£ç†å­—ç¬¦ä¸²
6. **ç¼–ç å¤„ç†**: æ­£ç¡®å¤„ç†ä¸­æ–‡ç¼–ç 

#### æ•°æ®æå–æ ‡å‡†
```python
from dataclasses import dataclass
from datetime import datetime
from typing import List, Optional

@dataclass
class ScrapedData:
    url: str
    title: str
    content: str
    images: List[str]
    links: List[str]
    scraped_at: datetime
    page_type: str
    language: str = 'zh-CN'
```

#### æ–‡ä»¶ç»„ç»‡ç»“æ„
```
web/
â”œâ”€â”€ web_crawler_v3.py          # ä¸»çˆ¬è™«è„šæœ¬
â”œâ”€â”€ requirements.txt           # Pythonä¾èµ–
â”œâ”€â”€ assaybio_v5/              # è¾“å‡ºç›®å½•
â”œâ”€â”€ logs/                     # çˆ¬å–æ—¥å¿—
â””â”€â”€ config/                   # é…ç½®æ–‡ä»¶
```

## å¼€å‘å·¥ä½œæµç¨‹

### 1. å¼€å‘ç¯å¢ƒè®¾ç½®
```bash
# å‰ç«¯ä¾èµ–
pnpm install

# Pythonçˆ¬è™«ä¾èµ–
cd web
pip install -r requirements.txt
```

### 2. å¸¸ç”¨å¼€å‘å‘½ä»¤
```bash
# ç½‘ç«™å¼€å‘
pnpm dev                    # å¯åŠ¨ç½‘ç«™å¼€å‘æœåŠ¡å™¨

# å†…å®¹æŠ“å–
cd web
python web_crawler_v3.py http://www.assaybio.cn/info.aspx?id=00070001 -d 10

# æ„å»ºå’Œéƒ¨ç½²
pnpm build                  # æ„å»ºç½‘ç«™
pnpm deploy                 # Dockeréƒ¨ç½²
```

### 3. æµ‹è¯•å’Œè´¨é‡æ§åˆ¶
```bash
# è¿è¡Œæµ‹è¯•
pnpm test

# ä»£ç æ£€æŸ¥
pnpm lint

# ç±»å‹æ£€æŸ¥
pnpm typecheck
```

## ä»£ç è´¨é‡æ ‡å‡†

### Pythonä»£ç è¦æ±‚
- ä½¿ç”¨ç±»å‹æ³¨è§£
- éµå¾ªPEP 8ä»£ç è§„èŒƒ
- ä½¿ç”¨dataclassæˆ–pydanticè¿›è¡Œæ•°æ®å»ºæ¨¡
- é€‚å½“çš„å¼‚å¸¸å¤„ç†

### Pythonçˆ¬è™«ä»£ç è§„èŒƒ
1. **å§‹ç»ˆä½¿ç”¨Sessionç®¡ç†è¿æ¥**
2. **å®ç°é€‚å½“çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶**
3. **éµå¾ªç½‘ç«™çš„çˆ¬è™«ç¤¼ä»ª**
4. **æ­£ç¡®å¤„ç†ç¼–ç é—®é¢˜**
5. **é¿å…ç¡¬ç¼–ç å»¶æ—¶ï¼Œä½¿ç”¨é…ç½®åŒ–çš„å»¶æ—¶è®¾ç½®**

### é”™è¯¯å¤„ç†ç­–ç•¥
```python
try:
    response = session.get(url, timeout=30)
    response.raise_for_status()
except requests.exceptions.Timeout:
    print(f'Request timeout for {url}')
    return None
except requests.exceptions.RequestException as e:
    print(f'Request failed for {url}: {e}')
    return None
```

## æ€§èƒ½å’Œèµ„æºç®¡ç†

### HTTPè¿æ¥ç®¡ç†
```python
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

class WebCrawler:
    def __init__(self):
        self.session = requests.Session()
        
        # é…ç½®é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504]
        )
        
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
    
    def cleanup(self):
        if self.session:
            self.session.close()
```

### å†…å­˜ä¼˜åŒ–
- ä½¿ç”¨Sessionå¤ç”¨HTTPè¿æ¥
- åŠæ—¶æ¸…ç†å¤§å‹å“åº”å¯¹è±¡
- é¿å…åœ¨å¾ªç¯ä¸­åˆ›å»ºè¿‡å¤šçš„Sessionå®ä¾‹
- ä½¿ç”¨ç”Ÿæˆå™¨å¤„ç†å¤§é‡æ•°æ®

## éƒ¨ç½²å’Œç›‘æ§

### Dockeré…ç½®
é¡¹ç›®å·²é…ç½®Dockeréƒ¨ç½²ï¼Œç¡®ä¿Pythonçˆ¬è™«ç¯å¢ƒæ­£å¸¸è¿è¡Œï¼š

```dockerfile
# ç¡®ä¿å®‰è£…Pythonä¾èµ–
COPY requirements.txt .
RUN pip install -r requirements.txt
```

### ç›‘æ§æŒ‡æ ‡
- æŠ“å–æˆåŠŸç‡
- HTTPå“åº”æ—¶é—´
- é”™è¯¯ç‡å’Œé‡è¯•æ¬¡æ•°
- èµ„æºä½¿ç”¨æƒ…å†µ

## å®‰å…¨è€ƒè™‘

### ç½‘é¡µæŠ“å–å®‰å…¨
1. **éµå¾ªç›®æ ‡ç½‘ç«™çš„robots.txt**
2. **å®ç°é€‚å½“çš„è¯·æ±‚é¢‘ç‡é™åˆ¶**
3. **ä½¿ç”¨åˆé€‚çš„User-Agent**
4. **é¿å…æš´éœ²æ•æ„Ÿä¿¡æ¯åœ¨æ—¥å¿—ä¸­**

### æ•°æ®å®‰å…¨
- æ•æ„Ÿæ•°æ®åŠ å¯†å­˜å‚¨
- å®šæœŸæ¸…ç†ä¸´æ—¶æ–‡ä»¶
- è®¿é—®æ—¥å¿—è®°å½•å’Œå®¡è®¡

## æ•…éšœæ’æŸ¥

### å¸¸è§Pythonçˆ¬è™«é—®é¢˜
1. **è¿æ¥è¶…æ—¶**: æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œè¶…æ—¶è®¾ç½®
2. **ç¼–ç é”™è¯¯**: æ­£ç¡®å¤„ç†å“åº”ç¼–ç 
3. **åçˆ¬æœºåˆ¶**: è°ƒæ•´è¯·æ±‚é¢‘ç‡å’ŒUser-Agent
4. **å†…å­˜å ç”¨**: åŠæ—¶æ¸…ç†å¤§å‹å¯¹è±¡å’Œä½¿ç”¨ç”Ÿæˆå™¨

### è°ƒè¯•æŠ€å·§
```python
import logging

# å¯ç”¨è°ƒè¯•æ—¥å¿—
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

# ä¿å­˜å“åº”å†…å®¹ç”¨äºè°ƒè¯•
with open('debug_response.html', 'w', encoding='utf-8') as f:
    f.write(response.text)

# æ‰“å°è¯·æ±‚è¯¦æƒ…
print(f"Request URL: {response.url}")
print(f"Status Code: {response.status_code}")
print(f"Headers: {response.headers}")
```

---

**é‡è¦æé†’**: æœ¬é¡¹ç›®çš„æ‰€æœ‰ç½‘é¡µæŠ“å–éœ€æ±‚å¿…é¡»ä¸¥æ ¼ä½¿ç”¨Python + requests + BeautifulSoupæŠ€æœ¯æ ˆå®ç°ã€‚ä¸¥ç¦ä½¿ç”¨ä»»ä½•æµè§ˆå™¨è‡ªåŠ¨åŒ–å·¥å…·ã€‚