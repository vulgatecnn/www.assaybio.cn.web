#!/usr/bin/env python3
"""
Web Crawler Tool - å®Œæ•´ç½‘ç«™å†…å®¹ä¸‹è½½å·¥å…·
åŠŸèƒ½ï¼šä¸‹è½½ç½‘ç«™å®Œæ•´å†…å®¹ï¼ŒåŒ…æ‹¬HTMLã€å›¾ç‰‡ã€æ–‡æ¡£ç­‰æ‰€æœ‰æ–‡ä»¶
å†…éƒ¨é“¾æ¥ä¸‹è½½åˆ°æœ¬åœ°ï¼Œå¤–éƒ¨é“¾æ¥ä¿æŒåŸæ ·
"""

import argparse
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, urlunparse, quote
import json
import csv
import os
import time
import sys
from typing import List, Dict, Set, Optional, Tuple
import logging
import hashlib
import mimetypes
from pathlib import Path
import shutil
import re
from concurrent.futures import ThreadPoolExecutor, as_completed


class WebCrawler:
    def __init__(self, base_url: str, output_dir: str = "downloaded_site", max_depth: int = 1, 
                 delay: float = 1.0, max_workers: int = 5):
        """
        åˆå§‹åŒ–ç½‘ç»œçˆ¬è™«
        
        Args:
            base_url: åŸºç¡€URL
            output_dir: è¾“å‡ºç›®å½•
            max_depth: æœ€å¤§çˆ¬å–æ·±åº¦
            delay: è¯·æ±‚é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰
            max_workers: å¹¶å‘ä¸‹è½½çº¿ç¨‹æ•°
        """
        self.base_url = base_url
        self.base_domain = urlparse(base_url).netloc
        self.max_depth = max_depth
        self.delay = delay
        self.max_workers = max_workers
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        self.html_dir = self.output_dir / "html"
        self.assets_dir = self.output_dir / "assets"
        self.images_dir = self.assets_dir / "images"
        self.documents_dir = self.assets_dir / "documents"
        self.css_dir = self.assets_dir / "css"
        self.js_dir = self.assets_dir / "js"
        
        for dir_path in [self.html_dir, self.assets_dir, self.images_dir, 
                        self.documents_dir, self.css_dir, self.js_dir]:
            dir_path.mkdir(exist_ok=True)
        
        # å­˜å‚¨ç»“æœ
        self.internal_links: Set[str] = set()
        self.external_links: Set[str] = set()
        self.downloaded_files: Dict[str, str] = {}  # URL -> local_path
        self.visited_urls: Set[str] = set()
        self.failed_downloads: Set[str] = set()
        
        # æ–‡ä»¶ç±»å‹æ˜ å°„
        self.file_extensions = {
            'images': {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.svg', '.webp', '.ico'},
            'documents': {'.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx', '.txt', '.rtf'},
            'css': {'.css'},
            'js': {'.js'},
            'html': {'.html', '.htm', '.php', '.asp', '.jsp'}
        }
        
        # é…ç½®è¯·æ±‚å¤´
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': '*/*',
            'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive'
        }
        
        # é…ç½®æ—¥å¿—
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        self.logger = logging.getLogger(__name__)

    def is_valid_url(self, url: str) -> bool:
        """æ£€æŸ¥URLæ˜¯å¦æœ‰æ•ˆ"""
        try:
            parsed = urlparse(url)
            return bool(parsed.netloc) and bool(parsed.scheme)
        except:
            return False

    def normalize_url(self, url: str) -> str:
        """æ ‡å‡†åŒ–URL"""
        parsed = urlparse(url)
        # ç§»é™¤fragmentéƒ¨åˆ†
        normalized = urlunparse((parsed.scheme, parsed.netloc, parsed.path, 
                               parsed.params, parsed.query, ''))
        return normalized

    def is_internal_url(self, url: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå†…éƒ¨é“¾æ¥"""
        try:
            parsed = urlparse(url)
            return parsed.netloc == self.base_domain or parsed.netloc == ''
        except:
            return False

    def get_file_extension(self, url: str) -> str:
        """è·å–æ–‡ä»¶æ‰©å±•å"""
        parsed = urlparse(url)
        path = parsed.path.lower()
        if '.' in path:
            return Path(path).suffix
        return ''

    def get_file_type(self, url: str) -> str:
        """æ ¹æ®URLç¡®å®šæ–‡ä»¶ç±»å‹"""
        ext = self.get_file_extension(url)
        
        for file_type, extensions in self.file_extensions.items():
            if ext in extensions:
                return file_type
        
        # é»˜è®¤è¿”å›htmlç±»å‹
        return 'html'

    def generate_local_filename(self, url: str, content_type: str = None) -> Tuple[str, Path]:
        """ç”Ÿæˆæœ¬åœ°æ–‡ä»¶åå’Œå®Œæ•´è·¯å¾„"""
        parsed = urlparse(url)
        path = parsed.path
        
        # å¦‚æœè·¯å¾„ä¸ºç©ºæˆ–ä»¥/ç»“å°¾ï¼Œä½¿ç”¨index.html
        if not path or path.endswith('/'):
            filename = 'index.html'
            file_type = 'html'
        else:
            # æå–æ–‡ä»¶å
            filename = Path(path).name
            if not filename:
                filename = 'index.html'
            file_type = self.get_file_type(url)
        
        # å¦‚æœæ²¡æœ‰æ‰©å±•åï¼Œæ ¹æ®content-typeæ·»åŠ 
        if '.' not in filename and content_type:
            if 'text/html' in content_type:
                filename += '.html'
                file_type = 'html'
            elif 'text/css' in content_type:
                filename += '.css'
                file_type = 'css'
            elif 'application/javascript' in content_type:
                filename += '.js'
                file_type = 'js'
            elif content_type.startswith('image/'):
                ext = mimetypes.guess_extension(content_type)
                if ext:
                    filename += ext
                    file_type = 'images'
        
        # å¤„ç†ç‰¹æ®Šå­—ç¬¦
        filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
        
        # é€‰æ‹©ç›®æ ‡ç›®å½•
        if file_type == 'html':
            target_dir = self.html_dir
        elif file_type == 'images':
            target_dir = self.images_dir
        elif file_type == 'documents':
            target_dir = self.documents_dir
        elif file_type == 'css':
            target_dir = self.css_dir
        elif file_type == 'js':
            target_dir = self.js_dir
        else:
            target_dir = self.assets_dir
        
        # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
        counter = 1
        original_filename = filename
        while (target_dir / filename).exists():
            name, ext = os.path.splitext(original_filename)
            filename = f"{name}_{counter}{ext}"
            counter += 1
        
        full_path = target_dir / filename
        return filename, full_path

    def download_file(self, url: str, force_download: bool = False) -> Optional[str]:
        """ä¸‹è½½æ–‡ä»¶åˆ°æœ¬åœ°"""
        if url in self.downloaded_files and not force_download:
            return self.downloaded_files[url]
        
        if url in self.failed_downloads:
            return None
            
        try:
            self.logger.info(f"æ­£åœ¨ä¸‹è½½: {url}")
            response = requests.get(url, headers=self.headers, timeout=30, stream=True)
            response.raise_for_status()
            
            # è·å–å†…å®¹ç±»å‹
            content_type = response.headers.get('content-type', '')
            
            # ç”Ÿæˆæœ¬åœ°æ–‡ä»¶å
            filename, full_path = self.generate_local_filename(url, content_type)
            
            # ä¸‹è½½æ–‡ä»¶
            with open(full_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            
            # è®°å½•ä¸‹è½½çš„æ–‡ä»¶
            relative_path = os.path.relpath(full_path, self.output_dir)
            self.downloaded_files[url] = relative_path
            
            self.logger.info(f"ä¸‹è½½å®Œæˆ: {url} -> {relative_path}")
            return relative_path
            
        except requests.exceptions.RequestException as e:
            self.logger.error(f"ä¸‹è½½å¤±è´¥ {url}: {e}")
            self.failed_downloads.add(url)
            return None
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥ {url}: {e}")
            self.failed_downloads.add(url)
            return None

    def get_page_content(self, url: str) -> Optional[BeautifulSoup]:
        """è·å–é¡µé¢å†…å®¹"""
        try:
            self.logger.info(f"æ­£åœ¨è·å–é¡µé¢: {url}")
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            # æ£€æŸ¥å†…å®¹ç±»å‹
            content_type = response.headers.get('content-type', '').lower()
            
            # å¦‚æœæ˜¯HTMLé¡µé¢ï¼Œè§£æå¹¶è¿”å›BeautifulSoupå¯¹è±¡
            if 'text/html' in content_type:
                soup = BeautifulSoup(response.content, 'html.parser')
                return soup
            else:
                # å¦‚æœä¸æ˜¯HTMLï¼Œç›´æ¥ä¸‹è½½æ–‡ä»¶
                if self.is_internal_url(url):
                    self.download_file(url)
                return None
                
        except requests.exceptions.RequestException as e:
            self.logger.error(f"è·å–é¡µé¢å¤±è´¥ {url}: {e}")
            return None
        except Exception as e:
            self.logger.error(f"è§£æé¡µé¢å¤±è´¥ {url}: {e}")
            return None

    def extract_all_resources(self, soup: BeautifulSoup, base_url: str) -> Dict[str, List[str]]:
        """æå–é¡µé¢ä¸­çš„æ‰€æœ‰èµ„æºé“¾æ¥"""
        resources = {
            'links': [],
            'images': [],
            'css': [],
            'js': [],
            'documents': [],
            'other': []
        }
        
        # æå–<a>æ ‡ç­¾é“¾æ¥
        for link in soup.find_all('a', href=True):
            href = link['href'].strip()
            if not href or href.startswith('#') or href.startswith('mailto:') or href.startswith('tel:'):
                continue
                
            absolute_url = urljoin(base_url, href)
            absolute_url = self.normalize_url(absolute_url)
            
            if self.is_valid_url(absolute_url):
                resources['links'].append(absolute_url)
        
        # æå–å›¾ç‰‡èµ„æº
        for img in soup.find_all('img'):
            for attr in ['src', 'data-src', 'data-original']:
                if img.get(attr):
                    src = img[attr].strip()
                    if src:
                        absolute_url = urljoin(base_url, src)
                        absolute_url = self.normalize_url(absolute_url)
                        if self.is_valid_url(absolute_url):
                            resources['images'].append(absolute_url)
                        break
        
        # æå–CSSæ–‡ä»¶
        for link in soup.find_all('link', rel='stylesheet'):
            if link.get('href'):
                href = link['href'].strip()
                absolute_url = urljoin(base_url, href)
                absolute_url = self.normalize_url(absolute_url)
                if self.is_valid_url(absolute_url):
                    resources['css'].append(absolute_url)
        
        # æå–JavaScriptæ–‡ä»¶
        for script in soup.find_all('script', src=True):
            src = script['src'].strip()
            absolute_url = urljoin(base_url, src)
            absolute_url = self.normalize_url(absolute_url)
            if self.is_valid_url(absolute_url):
                resources['js'].append(absolute_url)
        
        # æå–å…¶ä»–èµ„æºï¼ˆvideo, audio, source, iframeç­‰ï¼‰
        for tag_name, attr in [('video', 'src'), ('audio', 'src'), ('source', 'src'), 
                              ('iframe', 'src'), ('embed', 'src'), ('object', 'data')]:
            for element in soup.find_all(tag_name):
                if element.get(attr):
                    src = element[attr].strip()
                    absolute_url = urljoin(base_url, src)
                    absolute_url = self.normalize_url(absolute_url)
                    if self.is_valid_url(absolute_url):
                        resources['other'].append(absolute_url)
        
        # æå–CSSä¸­çš„èƒŒæ™¯å›¾ç‰‡
        for element in soup.find_all(style=True):
            style = element['style']
            if 'url(' in style:
                matches = re.findall(r'url\(["\']?(.*?)["\']?\)', style)
                for match in matches:
                    absolute_url = urljoin(base_url, match.strip())
                    absolute_url = self.normalize_url(absolute_url)
                    if self.is_valid_url(absolute_url):
                        resources['images'].append(absolute_url)
        
        # å»é‡
        for key in resources:
            resources[key] = list(set(resources[key]))
            
        return resources

    def update_html_content(self, soup: BeautifulSoup, base_url: str) -> str:
        """æ›´æ–°HTMLå†…å®¹ï¼Œå°†å†…éƒ¨èµ„æºé“¾æ¥æŒ‡å‘æœ¬åœ°æ–‡ä»¶"""
        
        # æ›´æ–°å›¾ç‰‡é“¾æ¥
        for img in soup.find_all('img'):
            for attr in ['src', 'data-src', 'data-original']:
                if img.get(attr):
                    original_url = urljoin(base_url, img[attr])
                    if self.is_internal_url(original_url) and original_url in self.downloaded_files:
                        img[attr] = self.downloaded_files[original_url]
                    break
        
        # æ›´æ–°CSSé“¾æ¥
        for link in soup.find_all('link', rel='stylesheet'):
            if link.get('href'):
                original_url = urljoin(base_url, link['href'])
                if self.is_internal_url(original_url) and original_url in self.downloaded_files:
                    link['href'] = self.downloaded_files[original_url]
        
        # æ›´æ–°JavaScripté“¾æ¥
        for script in soup.find_all('script', src=True):
            original_url = urljoin(base_url, script['src'])
            if self.is_internal_url(original_url) and original_url in self.downloaded_files:
                script['src'] = self.downloaded_files[original_url]
        
        # æ›´æ–°<a>æ ‡ç­¾é“¾æ¥
        for link in soup.find_all('a', href=True):
            original_url = urljoin(base_url, link['href'])
            if self.is_internal_url(original_url) and original_url in self.downloaded_files:
                link['href'] = self.downloaded_files[original_url]
        
        # æ›´æ–°å…¶ä»–èµ„æºé“¾æ¥
        for tag_name, attr in [('video', 'src'), ('audio', 'src'), ('source', 'src'), 
                              ('iframe', 'src'), ('embed', 'src'), ('object', 'data')]:
            for element in soup.find_all(tag_name):
                if element.get(attr):
                    original_url = urljoin(base_url, element[attr])
                    if self.is_internal_url(original_url) and original_url in self.downloaded_files:
                        element[attr] = self.downloaded_files[original_url]
        
        return str(soup)

    def crawl_page(self, url: str, depth: int = 0) -> None:
        """çˆ¬å–å•ä¸ªé¡µé¢"""
        if depth > self.max_depth or url in self.visited_urls:
            return
            
        self.visited_urls.add(url)
        
        # è·å–é¡µé¢å†…å®¹
        soup = self.get_page_content(url)
        if not soup:
            return
            
        # æå–æ‰€æœ‰èµ„æº
        resources = self.extract_all_resources(soup, url)
        
        # åˆ†ç±»å¤„ç†èµ„æº
        all_urls_to_download = []
        
        # å¤„ç†é“¾æ¥
        for link in resources['links']:
            if self.is_internal_url(link):
                self.internal_links.add(link)
            else:
                self.external_links.add(link)
        
        # æ”¶é›†æ‰€æœ‰éœ€è¦ä¸‹è½½çš„å†…éƒ¨èµ„æº
        for resource_type in ['images', 'css', 'js', 'other']:
            for resource_url in resources[resource_type]:
                if self.is_internal_url(resource_url):
                    all_urls_to_download.append(resource_url)
                else:
                    self.external_links.add(resource_url)
        
        # å¹¶å‘ä¸‹è½½æ‰€æœ‰èµ„æº
        if all_urls_to_download:
            self.logger.info(f"å¼€å§‹ä¸‹è½½ {len(all_urls_to_download)} ä¸ªèµ„æºæ–‡ä»¶")
            self.download_resources_parallel(all_urls_to_download)
        
        # æ›´æ–°HTMLå†…å®¹å¹¶ä¿å­˜
        updated_html = self.update_html_content(soup, url)
        html_filename, html_path = self.generate_local_filename(url, 'text/html')
        
        try:
            with open(html_path, 'w', encoding='utf-8') as f:
                f.write(updated_html)
            
            relative_path = os.path.relpath(html_path, self.output_dir)
            self.downloaded_files[url] = relative_path
            self.logger.info(f"HTMLé¡µé¢ä¿å­˜: {url} -> {relative_path}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜HTMLé¡µé¢å¤±è´¥ {url}: {e}")
        
        self.logger.info(f"é¡µé¢ {url} - å‘ç° {len(resources['links'])} ä¸ªé“¾æ¥, "
                        f"{len(all_urls_to_download)} ä¸ªå†…éƒ¨èµ„æº")
        
        # å¦‚æœéœ€è¦æ·±åº¦çˆ¬å–ï¼Œç»§ç»­çˆ¬å–åŒåŸŸåä¸‹çš„HTMLé¡µé¢
        if depth < self.max_depth:
            internal_html_links = [link for link in resources['links'] 
                                 if self.is_internal_url(link)
                                 and link not in self.visited_urls
                                 and self.get_file_type(link) == 'html']
            
            for link in internal_html_links[:10]:  # é™åˆ¶æ¯é¡µæœ€å¤šçˆ¬å–10ä¸ªå­é¡µé¢
                time.sleep(self.delay)
                self.crawl_page(link, depth + 1)
    
    def download_resources_parallel(self, urls: List[str]) -> None:
        """å¹¶å‘ä¸‹è½½èµ„æº"""
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_url = {executor.submit(self.download_file, url): url for url in urls}
            
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    result = future.result()
                    if result:
                        self.logger.debug(f"èµ„æºä¸‹è½½å®Œæˆ: {url}")
                except Exception as e:
                    self.logger.error(f"å¹¶å‘ä¸‹è½½å¤±è´¥ {url}: {e}")

    def crawl(self) -> Dict:
        """å¼€å§‹çˆ¬å–"""
        self.logger.info(f"å¼€å§‹å®Œæ•´ä¸‹è½½ç½‘ç«™: {self.base_url}")
        self.logger.info(f"æœ€å¤§æ·±åº¦: {self.max_depth}, å»¶æ—¶: {self.delay}ç§’")
        self.logger.info(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        
        start_time = time.time()
        
        try:
            self.crawl_page(self.base_url)
        except KeyboardInterrupt:
            self.logger.info("ç”¨æˆ·ä¸­æ–­ä¸‹è½½...")
        
        end_time = time.time()
        
        result = {
            'base_url': self.base_url,
            'output_directory': str(self.output_dir),
            'crawl_time': end_time - start_time,
            'total_pages_visited': len(self.visited_urls),
            'total_internal_links': len(self.internal_links),
            'total_external_links': len(self.external_links),
            'total_downloaded_files': len(self.downloaded_files),
            'total_failed_downloads': len(self.failed_downloads),
            'internal_links': sorted(list(self.internal_links)),
            'external_links': sorted(list(self.external_links)),
            'downloaded_files': dict(self.downloaded_files),
            'failed_downloads': sorted(list(self.failed_downloads)),
            'visited_pages': sorted(list(self.visited_urls))
        }
        
        self.logger.info(f"ä¸‹è½½å®Œæˆ! ç”¨æ—¶: {result['crawl_time']:.2f}ç§’")
        self.logger.info(f"è®¿é—®é¡µé¢: {result['total_pages_visited']} ä¸ª")
        self.logger.info(f"å†…éƒ¨é“¾æ¥: {result['total_internal_links']} ä¸ª")
        self.logger.info(f"å¤–éƒ¨é“¾æ¥: {result['total_external_links']} ä¸ª")
        self.logger.info(f"ä¸‹è½½æ–‡ä»¶: {result['total_downloaded_files']} ä¸ª")
        self.logger.info(f"ä¸‹è½½å¤±è´¥: {result['total_failed_downloads']} ä¸ª")
        
        # åˆ›å»ºç´¢å¼•é¡µé¢
        self.create_index_page(result)
        
        return result
    
    def create_index_page(self, result: Dict) -> None:
        """åˆ›å»ºç´¢å¼•é¡µé¢"""
        try:
            index_html = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç½‘ç«™ä¸‹è½½å®Œæˆ - {self.base_domain}</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }}
        h1, h2 {{ color: #333; }}
        .stats {{ background: #f4f4f4; padding: 20px; border-radius: 5px; margin: 20px 0; }}
        .file-list {{ max-height: 300px; overflow-y: auto; border: 1px solid #ddd; padding: 10px; }}
        .external-link {{ color: #e74c3c; }}
        .internal-link {{ color: #27ae60; }}
        .failed {{ color: #e74c3c; }}
        table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f2f2f2; }}
    </style>
</head>
<body>
    <h1>ç½‘ç«™ä¸‹è½½å®Œæˆ</h1>
    <div class="stats">
        <h2>ä¸‹è½½ç»Ÿè®¡</h2>
        <p><strong>åŸå§‹ç½‘ç«™:</strong> <a href="{result['base_url']}" target="_blank">{result['base_url']}</a></p>
        <p><strong>ä¸‹è½½æ—¶é—´:</strong> {result['crawl_time']:.2f} ç§’</p>
        <p><strong>è®¿é—®é¡µé¢:</strong> {result['total_pages_visited']} ä¸ª</p>
        <p><strong>ä¸‹è½½æ–‡ä»¶:</strong> {result['total_downloaded_files']} ä¸ª</p>
        <p><strong>å†…éƒ¨é“¾æ¥:</strong> {result['total_internal_links']} ä¸ª</p>
        <p><strong>å¤–éƒ¨é“¾æ¥:</strong> {result['total_external_links']} ä¸ª</p>
        <p><strong>ä¸‹è½½å¤±è´¥:</strong> {result['total_failed_downloads']} ä¸ª</p>
    </div>

    <h2>å·²ä¸‹è½½çš„é¡µé¢</h2>
    <div class="file-list">
        <ul>
"""
            
            # æ·»åŠ HTMLé¡µé¢é“¾æ¥
            html_files = [(url, path) for url, path in result['downloaded_files'].items() 
                         if path.startswith('html/')]
            
            for original_url, local_path in sorted(html_files):
                index_html += f'<li><a href="{local_path}" target="_blank">{original_url}</a></li>\n'
            
            index_html += """
        </ul>
    </div>

    <h2>å¤–éƒ¨é“¾æ¥ (æœªä¸‹è½½)</h2>
    <div class="file-list">
        <ul>
"""
            
            for external_link in sorted(result['external_links'])[:50]:  # æœ€å¤šæ˜¾ç¤º50ä¸ªå¤–éƒ¨é“¾æ¥
                index_html += f'<li><a href="{external_link}" target="_blank" class="external-link">{external_link}</a></li>\n'
            
            index_html += """
        </ul>
    </div>

    <h2>æ–‡ä»¶ç»“æ„</h2>
    <p>ä¸‹è½½çš„æ–‡ä»¶æŒ‰ä»¥ä¸‹ç»“æ„ç»„ç»‡:</p>
    <ul>
        <li><strong>html/</strong> - HTMLé¡µé¢æ–‡ä»¶</li>
        <li><strong>assets/</strong> - èµ„æºæ–‡ä»¶
            <ul>
                <li><strong>images/</strong> - å›¾ç‰‡æ–‡ä»¶</li>
                <li><strong>css/</strong> - CSSæ ·å¼æ–‡ä»¶</li>
                <li><strong>js/</strong> - JavaScriptæ–‡ä»¶</li>
                <li><strong>documents/</strong> - æ–‡æ¡£æ–‡ä»¶ (PDF, DOCç­‰)</li>
            </ul>
        </li>
    </ul>
</body>
</html>
"""
            
            index_path = self.output_dir / 'index.html'
            with open(index_path, 'w', encoding='utf-8') as f:
                f.write(index_html)
                
            self.logger.info(f"ç´¢å¼•é¡µé¢å·²åˆ›å»º: {index_path}")
            
        except Exception as e:
            self.logger.error(f"åˆ›å»ºç´¢å¼•é¡µé¢å¤±è´¥: {e}")

    def save_results(self, result: Dict, output_format: str, output_file: str) -> None:
        """ä¿å­˜ç»“æœ"""
        try:
            if output_format.lower() == 'json':
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(result, f, ensure_ascii=False, indent=2)
                    
            elif output_format.lower() == 'csv':
                with open(output_file, 'w', newline='', encoding='utf-8') as f:
                    writer = csv.writer(f)
                    
                    # å†™å…¥æ¦‚è¦ä¿¡æ¯
                    writer.writerow(['æ¦‚è¦ä¿¡æ¯'])
                    writer.writerow(['åŸºç¡€URL', result['base_url']])
                    writer.writerow(['è¾“å‡ºç›®å½•', result['output_directory']])
                    writer.writerow(['ä¸‹è½½æ—¶é—´(ç§’)', f"{result['crawl_time']:.2f}"])
                    writer.writerow(['è®¿é—®é¡µé¢æ•°', result['total_pages_visited']])
                    writer.writerow(['ä¸‹è½½æ–‡ä»¶æ•°', result['total_downloaded_files']])
                    writer.writerow(['å†…éƒ¨é“¾æ¥æ•°', result['total_internal_links']])
                    writer.writerow(['å¤–éƒ¨é“¾æ¥æ•°', result['total_external_links']])
                    writer.writerow(['ä¸‹è½½å¤±è´¥æ•°', result['total_failed_downloads']])
                    writer.writerow([])
                    
                    # å†™å…¥å·²ä¸‹è½½æ–‡ä»¶
                    writer.writerow(['å·²ä¸‹è½½æ–‡ä»¶', 'æœ¬åœ°è·¯å¾„'])
                    for url, local_path in result['downloaded_files'].items():
                        writer.writerow([url, local_path])
                    writer.writerow([])
                    
                    # å†™å…¥å¤–éƒ¨é“¾æ¥
                    writer.writerow(['å¤–éƒ¨é“¾æ¥'])
                    for link in result['external_links']:
                        writer.writerow([link])
                    writer.writerow([])
                    
                    # å†™å…¥ä¸‹è½½å¤±è´¥çš„æ–‡ä»¶
                    if result['failed_downloads']:
                        writer.writerow(['ä¸‹è½½å¤±è´¥çš„æ–‡ä»¶'])
                        for url in result['failed_downloads']:
                            writer.writerow([url])
                        
            elif output_format.lower() == 'txt':
                with open(output_file, 'w', encoding='utf-8') as f:
                    f.write(f"ç½‘ç«™å®Œæ•´ä¸‹è½½ç»“æœ\n")
                    f.write(f"="*60 + "\n")
                    f.write(f"åŸºç¡€URL: {result['base_url']}\n")
                    f.write(f"è¾“å‡ºç›®å½•: {result['output_directory']}\n")
                    f.write(f"ä¸‹è½½æ—¶é—´: {result['crawl_time']:.2f}ç§’\n")
                    f.write(f"è®¿é—®é¡µé¢: {result['total_pages_visited']} ä¸ª\n")
                    f.write(f"ä¸‹è½½æ–‡ä»¶: {result['total_downloaded_files']} ä¸ª\n")
                    f.write(f"å†…éƒ¨é“¾æ¥: {result['total_internal_links']} ä¸ª\n")
                    f.write(f"å¤–éƒ¨é“¾æ¥: {result['total_external_links']} ä¸ª\n")
                    f.write(f"ä¸‹è½½å¤±è´¥: {result['total_failed_downloads']} ä¸ª\n\n")
                    
                    f.write("å·²ä¸‹è½½çš„æ–‡ä»¶:\n")
                    f.write("-"*40 + "\n")
                    for i, (url, local_path) in enumerate(result['downloaded_files'].items(), 1):
                        f.write(f"{i}. {url}\n   -> {local_path}\n")
                    
                    f.write("\nå¤–éƒ¨é“¾æ¥ (æœªä¸‹è½½):\n")
                    f.write("-"*40 + "\n")
                    for i, link in enumerate(result['external_links'], 1):
                        f.write(f"{i}. {link}\n")
                    
                    if result['failed_downloads']:
                        f.write("\nä¸‹è½½å¤±è´¥çš„æ–‡ä»¶:\n")
                        f.write("-"*40 + "\n")
                        for i, url in enumerate(result['failed_downloads'], 1):
                            f.write(f"{i}. {url}\n")
                        
            self.logger.info(f"è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")


def main():
    parser = argparse.ArgumentParser(description='å®Œæ•´ç½‘ç«™å†…å®¹ä¸‹è½½å·¥å…·')
    parser.add_argument('url', help='è¦ä¸‹è½½çš„ç½‘ç«™URL')
    parser.add_argument('-d', '--depth', type=int, default=1, help='çˆ¬å–æ·±åº¦ (é»˜è®¤: 1)')
    parser.add_argument('-o', '--output-dir', default='downloaded_site', help='è¾“å‡ºç›®å½• (é»˜è®¤: downloaded_site)')
    parser.add_argument('-f', '--format', choices=['json', 'csv', 'txt'], default='json', 
                       help='ç»“æœæ–‡ä»¶è¾“å‡ºæ ¼å¼ (é»˜è®¤: json)')
    parser.add_argument('--delay', type=float, default=1.0, help='è¯·æ±‚é—´éš”æ—¶é—´(ç§’) (é»˜è®¤: 1.0)')
    parser.add_argument('--max-workers', type=int, default=5, help='å¹¶å‘ä¸‹è½½çº¿ç¨‹æ•° (é»˜è®¤: 5)')
    parser.add_argument('-v', '--verbose', action='store_true', help='è¯¦ç»†è¾“å‡º')
    parser.add_argument('--report', default='download_report', help='ä¸‹è½½æŠ¥å‘Šæ–‡ä»¶åå‰ç¼€ (é»˜è®¤: download_report)')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # éªŒè¯URLæ ¼å¼
    if not args.url.startswith(('http://', 'https://')):
        args.url = 'https://' + args.url
    
    try:
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        crawler = WebCrawler(
            base_url=args.url, 
            output_dir=args.output_dir,
            max_depth=args.depth, 
            delay=args.delay,
            max_workers=args.max_workers
        )
        
        print(f"å¼€å§‹ä¸‹è½½ç½‘ç«™: {args.url}")
        print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
        print(f"æœ€å¤§æ·±åº¦: {args.depth}")
        print(f"å¹¶å‘çº¿ç¨‹: {args.max_workers}")
        print("-" * 50)
        
        # å¼€å§‹ä¸‹è½½
        result = crawler.crawl()
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = f"{args.report}.{args.format}"
        crawler.save_results(result, args.format, report_file)
        
        # æ‰“å°æ¦‚è¦ä¿¡æ¯
        print("\n" + "="*60)
        print("ä¸‹è½½å®Œæˆ!")
        print("="*60)
        print(f"åŸå§‹ç½‘ç«™: {result['base_url']}")
        print(f"è¾“å‡ºç›®å½•: {result['output_directory']}")
        print(f"ä¸‹è½½è€—æ—¶: {result['crawl_time']:.2f}ç§’")
        print(f"è®¿é—®é¡µé¢: {result['total_pages_visited']} ä¸ª")
        print(f"ä¸‹è½½æ–‡ä»¶: {result['total_downloaded_files']} ä¸ª")
        print(f"å†…éƒ¨é“¾æ¥: {result['total_internal_links']} ä¸ª") 
        print(f"å¤–éƒ¨é“¾æ¥: {result['total_external_links']} ä¸ª")
        if result['total_failed_downloads'] > 0:
            print(f"ä¸‹è½½å¤±è´¥: {result['total_failed_downloads']} ä¸ª")
        
        print(f"\næ–‡ä»¶ç»“æ„:")
        print(f"â”œâ”€â”€ {args.output_dir}/")
        print(f"â”‚   â”œâ”€â”€ index.html (ç´¢å¼•é¡µé¢)")
        print(f"â”‚   â”œâ”€â”€ html/ (ç½‘é¡µæ–‡ä»¶)")
        print(f"â”‚   â””â”€â”€ assets/ (èµ„æºæ–‡ä»¶)")
        print(f"â”‚       â”œâ”€â”€ images/ (å›¾ç‰‡)")
        print(f"â”‚       â”œâ”€â”€ css/ (æ ·å¼è¡¨)")
        print(f"â”‚       â”œâ”€â”€ js/ (è„šæœ¬æ–‡ä»¶)")
        print(f"â”‚       â””â”€â”€ documents/ (æ–‡æ¡£)")
        print(f"â””â”€â”€ {report_file} (è¯¦ç»†æŠ¥å‘Š)")
        
        print(f"\nğŸ‘† è¯·æ‰“å¼€ {args.output_dir}/index.html æŸ¥çœ‹ä¸‹è½½çš„ç½‘ç«™")
        
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­ä¸‹è½½")
        sys.exit(1)
    except Exception as e:
        print(f"ç¨‹åºæ‰§è¡Œé”™è¯¯: {e}")
        logging.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        sys.exit(1)


if __name__ == '__main__':
    main()