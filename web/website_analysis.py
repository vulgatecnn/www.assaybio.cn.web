#!/usr/bin/env python3
"""
å…¨é¢çš„ç½‘ç«™ç»“æ„åˆ†æå·¥å…·
åˆ†æåŸå§‹ç½‘ç«™å’Œæœ¬åœ°ä¸‹è½½ç‰ˆæœ¬çš„å·®å¼‚
"""
import requests
from bs4 import BeautifulSoup
import os
from pathlib import Path
import re
from urllib.parse import urljoin, urlparse
import json
from datetime import datetime

class WebsiteAnalyzer:
    def __init__(self, base_url="http://www.assaybio.cn", local_dir="assaybio_v4"):
        self.base_url = base_url
        self.local_dir = Path(local_dir)
        self.original_pages = {}
        self.local_files = {}
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
    def analyze_original_website(self):
        """åˆ†æåŸå§‹ç½‘ç«™ç»“æ„"""
        print("æ­£åœ¨åˆ†æåŸå§‹ç½‘ç«™ç»“æ„...")
        
        # ä¸»è¦é¡µé¢URLåˆ—è¡¨
        main_pages = {
            "é¦–é¡µ": "/",
            "å…³äºæˆ‘ä»¬": "/info.aspx?id=00010001", 
            "å…³äºæˆ‘ä»¬2": "/info.aspx?id=00010002",
            "å…³äºæˆ‘ä»¬3": "/info.aspx?id=00010003", 
            "æ–‡çŒ®èµ„æ–™": "/info.aspx?id=00050001",
            "æ–‡çŒ®èµ„æ–™2": "/info.aspx?id=00050002",
            "æ–‡çŒ®èµ„æ–™3": "/info.aspx?id=00050003",
            "å¸‚åœºåŠ¨å‘": "/info.aspx?id=00020001",
            "äº§å“": "/info.aspx?id=00070001",
        }
        
        # äº§å“è¯¦æƒ…é¡µ
        product_pages = {
            "äº§å“è¯¦æƒ…1748": "/display.aspx?id=1748",
            "äº§å“è¯¦æƒ…1749": "/display.aspx?id=1749", 
            "äº§å“è¯¦æƒ…1750": "/display.aspx?id=1750",
        }
        
        all_pages = {**main_pages, **product_pages}
        
        for name, url_path in all_pages.items():
            full_url = urljoin(self.base_url, url_path)
            try:
                print(f"åˆ†æ: {name} - {full_url}")
                response = requests.get(full_url, headers=self.headers, timeout=10)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    self.original_pages[name] = {
                        'url': full_url,
                        'title': soup.title.string if soup.title else "",
                        'content_length': len(response.text),
                        'status': 'success',
                        'links_found': self.extract_links(soup, full_url)
                    }
                else:
                    self.original_pages[name] = {
                        'url': full_url, 
                        'status': f'error_{response.status_code}',
                        'title': '',
                        'content_length': 0,
                        'links_found': []
                    }
            except Exception as e:
                print(f"è®¿é—®å¤±è´¥ {full_url}: {e}")
                self.original_pages[name] = {
                    'url': full_url,
                    'status': f'error_{str(e)}',
                    'title': '',
                    'content_length': 0,
                    'links_found': []
                }
                
    def extract_links(self, soup, base_url):
        """æå–é¡µé¢ä¸­çš„é“¾æ¥"""
        links = []
        for a_tag in soup.find_all('a', href=True):
            href = a_tag['href'].strip()
            if href and not href.startswith('#') and not href.startswith('mailto:'):
                full_url = urljoin(base_url, href)
                if 'assaybio.cn' in full_url:
                    links.append(full_url)
        return list(set(links))
    
    def analyze_local_files(self):
        """åˆ†ææœ¬åœ°ä¸‹è½½çš„æ–‡ä»¶"""
        print("æ­£åœ¨åˆ†ææœ¬åœ°æ–‡ä»¶...")
        
        if not self.local_dir.exists():
            print(f"æœ¬åœ°ç›®å½•ä¸å­˜åœ¨: {self.local_dir}")
            return
            
        # HTMLæ–‡ä»¶
        html_files = list(self.local_dir.glob("*.html"))
        for html_file in html_files:
            with open(html_file, 'r', encoding='utf-8') as f:
                content = f.read()
                soup = BeautifulSoup(content, 'html.parser')
                
                self.local_files[html_file.name] = {
                    'path': str(html_file),
                    'title': soup.title.string if soup.title else "",
                    'content_length': len(content),
                    'file_size': html_file.stat().st_size,
                    'type': 'html'
                }
        
        # å›¾ç‰‡æ–‡ä»¶
        image_files = []
        for ext in ['*.jpg', '*.png', '*.gif']:
            image_files.extend(self.local_dir.glob(ext))
        
        for img_file in image_files:
            self.local_files[img_file.name] = {
                'path': str(img_file),
                'file_size': img_file.stat().st_size,
                'type': 'image'
            }
            
        # CSSå’ŒJSæ–‡ä»¶
        for ext in ['*.css', '*.js']:
            for file_path in self.local_dir.glob(ext):
                self.local_files[file_path.name] = {
                    'path': str(file_path),
                    'file_size': file_path.stat().st_size, 
                    'type': 'resource'
                }
    
    def create_mapping_table(self):
        """åˆ›å»ºåŸå§‹URLåˆ°æœ¬åœ°æ–‡ä»¶çš„æ˜ å°„è¡¨"""
        url_to_file_mapping = {
            "/": "index.html",
            "/info.aspx?id=00010001": "info.aspx_id_00010001.html",
            "/info.aspx?id=00010002": "info.aspx_id_00010002_1.html", 
            "/info.aspx?id=00010003": "info.aspx_id_00010003.html",
            "/info.aspx?id=00050001": "info.aspx_id_00050001.html",
            "/info.aspx?id=00050002": "info.aspx_id_00050002.html",
            "/info.aspx?id=00050003": "info.aspx_id_00050003.html", 
            "/info.aspx?id=00020001": "info.aspx_id_00020001.html",
            "/info.aspx?id=00070001": "info.aspx_id_00070001.html",
            "/display.aspx?id=1748": "display.aspx_id_1748.html",
            "/display.aspx?id=1749": "display.aspx_id_1749.html",
            "/display.aspx?id=1750": "display.aspx_id_1750.html",
        }
        return url_to_file_mapping
        
    def compare_content(self):
        """æ¯”è¾ƒå†…å®¹ä¸€è‡´æ€§"""
        print("æ­£åœ¨æ¯”è¾ƒå†…å®¹ä¸€è‡´æ€§...")
        mapping = self.create_mapping_table()
        comparison_results = {}
        
        for page_name, page_data in self.original_pages.items():
            if page_data['status'] != 'success':
                comparison_results[page_name] = {
                    'status': 'original_inaccessible',
                    'reason': page_data['status']
                }
                continue
                
            # æ‰¾åˆ°å¯¹åº”çš„æœ¬åœ°æ–‡ä»¶
            url_path = page_data['url'].replace(self.base_url, "")
            local_filename = mapping.get(url_path)
            
            if local_filename and local_filename in self.local_files:
                local_file_data = self.local_files[local_filename]
                
                # æ¯”è¾ƒæ ‡é¢˜
                title_match = page_data['title'] == local_file_data['title']
                
                # æ¯”è¾ƒå†…å®¹é•¿åº¦
                length_diff = abs(page_data['content_length'] - local_file_data['content_length'])
                length_similarity = 1 - (length_diff / max(page_data['content_length'], 1))
                
                comparison_results[page_name] = {
                    'status': 'found',
                    'local_file': local_filename,
                    'title_match': title_match,
                    'original_title': page_data['title'],
                    'local_title': local_file_data['title'], 
                    'original_length': page_data['content_length'],
                    'local_length': local_file_data['content_length'],
                    'length_similarity': length_similarity,
                    'assessment': 'good' if length_similarity > 0.9 and title_match else 'needs_review'
                }
            else:
                comparison_results[page_name] = {
                    'status': 'missing',
                    'expected_filename': local_filename,
                    'original_url': page_data['url']
                }
                
        return comparison_results
    
    def generate_report(self):
        """ç”Ÿæˆè¯¦ç»†çš„MDæ ¼å¼æŠ¥å‘Š"""
        print("æ­£åœ¨ç”Ÿæˆæ£€æŸ¥æŠ¥å‘Š...")
        
        self.analyze_original_website()
        self.analyze_local_files()
        comparison_results = self.compare_content()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = f"""# ç½‘ç«™ç»“æ„å¯¹æ¯”æ£€æŸ¥æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**åŸå§‹ç½‘ç«™**: {self.base_url}  
**æœ¬åœ°ç›®å½•**: {self.local_dir}

## 1. ç½‘ç«™ç»“æ„æ¦‚è§ˆ

### 1.1 åŸå§‹ç½‘ç«™åˆ†æç»“æœ
- **æ€»é¡µé¢æ•°**: {len(self.original_pages)}
- **æˆåŠŸè®¿é—®**: {sum(1 for p in self.original_pages.values() if p['status'] == 'success')}
- **è®¿é—®å¤±è´¥**: {sum(1 for p in self.original_pages.values() if p['status'] != 'success')}

### 1.2 æœ¬åœ°æ–‡ä»¶åˆ†æç»“æœ  
- **HTMLæ–‡ä»¶æ•°**: {len([f for f in self.local_files.values() if f.get('type') == 'html'])}
- **å›¾ç‰‡æ–‡ä»¶æ•°**: {len([f for f in self.local_files.values() if f.get('type') == 'image'])}
- **èµ„æºæ–‡ä»¶æ•°**: {len([f for f in self.local_files.values() if f.get('type') == 'resource'])}
- **æ€»æ–‡ä»¶æ•°**: {len(self.local_files)}

## 2. é¡µé¢å¯¹æ¯”è¯¦æƒ…

"""
        
        # æˆåŠŸå¯¹æ¯”çš„é¡µé¢
        successful_comparisons = [k for k, v in comparison_results.items() if v['status'] == 'found']
        if successful_comparisons:
            report += "### 2.1 âœ… æˆåŠŸä¸‹è½½çš„é¡µé¢\n\n"
            for page_name in successful_comparisons:
                result = comparison_results[page_name]
                status_icon = "âœ…" if result['assessment'] == 'good' else "âš ï¸"
                report += f"{status_icon} **{page_name}**\n"
                report += f"- æœ¬åœ°æ–‡ä»¶: `{result['local_file']}`\n"
                report += f"- æ ‡é¢˜åŒ¹é…: {'âœ…' if result['title_match'] else 'âŒ'}\n"
                report += f"- å†…å®¹ç›¸ä¼¼åº¦: {result['length_similarity']:.2%}\n"
                report += f"- åŸå§‹é•¿åº¦: {result['original_length']:,} å­—ç¬¦\n"
                report += f"- æœ¬åœ°é•¿åº¦: {result['local_length']:,} å­—ç¬¦\n\n"
        
        # ç¼ºå¤±çš„é¡µé¢
        missing_pages = [k for k, v in comparison_results.items() if v['status'] == 'missing']
        if missing_pages:
            report += "### 2.2 âŒ ç¼ºå¤±çš„é¡µé¢\n\n"
            for page_name in missing_pages:
                result = comparison_results[page_name]
                report += f"âŒ **{page_name}**\n"
                report += f"- åŸå§‹URL: `{result['original_url']}`\n"
                report += f"- æœŸæœ›æ–‡ä»¶å: `{result.get('expected_filename', 'æœªçŸ¥')}`\n\n"
        
        # è®¿é—®å¤±è´¥çš„é¡µé¢
        failed_pages = [k for k, v in comparison_results.items() if v['status'] == 'original_inaccessible']
        if failed_pages:
            report += "### 2.3 ğŸ”´ æ— æ³•è®¿é—®çš„åŸå§‹é¡µé¢\n\n"
            for page_name in failed_pages:
                result = comparison_results[page_name]
                report += f"ğŸ”´ **{page_name}**\n"
                report += f"- é”™è¯¯åŸå› : `{result['reason']}`\n\n"
        
        # æœ¬åœ°æ–‡ä»¶æ¸…å•
        report += "## 3. æœ¬åœ°æ–‡ä»¶è¯¦ç»†æ¸…å•\n\n"
        
        # HTMLæ–‡ä»¶
        html_files = [f for f, data in self.local_files.items() if data.get('type') == 'html']
        if html_files:
            report += "### 3.1 HTMLæ–‡ä»¶\n\n"
            for filename in sorted(html_files):
                data = self.local_files[filename]
                report += f"- `{filename}` - {data['file_size']:,} bytes\n"
                if data.get('title'):
                    report += f"  - æ ‡é¢˜: {data['title']}\n"
        
        # å›¾ç‰‡æ–‡ä»¶ 
        image_files = [f for f, data in self.local_files.items() if data.get('type') == 'image']
        if image_files:
            report += f"\n### 3.2 å›¾ç‰‡æ–‡ä»¶ ({len(image_files)} ä¸ª)\n\n"
            for filename in sorted(image_files):
                data = self.local_files[filename]
                report += f"- `{filename}` - {data['file_size']:,} bytes\n"
        
        # èµ„æºæ–‡ä»¶
        resource_files = [f for f, data in self.local_files.items() if data.get('type') == 'resource']
        if resource_files:
            report += f"\n### 3.3 èµ„æºæ–‡ä»¶ ({len(resource_files)} ä¸ª)\n\n"
            for filename in sorted(resource_files):
                data = self.local_files[filename]
                report += f"- `{filename}` - {data['file_size']:,} bytes\n"
        
        # ä¿®å¤å»ºè®®
        report += "\n## 4. ä¿®å¤å»ºè®®\n\n"
        
        needs_review = [k for k, v in comparison_results.items() if v.get('assessment') == 'needs_review']
        if needs_review:
            report += "### 4.1 éœ€è¦æ£€æŸ¥çš„é¡µé¢\n\n"
            for page_name in needs_review:
                result = comparison_results[page_name]
                report += f"- **{page_name}**: "
                if not result['title_match']:
                    report += "æ ‡é¢˜ä¸åŒ¹é… "
                if result['length_similarity'] < 0.9:
                    report += f"å†…å®¹å·®å¼‚è¾ƒå¤§({result['length_similarity']:.1%}) "
                report += "\n"
        
        if missing_pages:
            report += "\n### 4.2 ç¼ºå¤±é¡µé¢å¤„ç†\n\n"
            report += "å»ºè®®é‡æ–°è¿è¡Œçˆ¬è™«ä¸‹è½½ä»¥ä¸‹é¡µé¢:\n"
            for page_name in missing_pages:
                result = comparison_results[page_name]
                report += f"- {result['original_url']}\n"
        
        # æ€»ç»“
        report += f"\n## 5. æ€»ç»“\n\n"
        report += f"- âœ… æˆåŠŸä¸‹è½½é¡µé¢: {len(successful_comparisons)} ä¸ª\n"
        report += f"- âŒ ç¼ºå¤±é¡µé¢: {len(missing_pages)} ä¸ª\n"
        report += f"- ğŸ”´ æ— æ³•è®¿é—®é¡µé¢: {len(failed_pages)} ä¸ª\n"
        report += f"- âš ï¸ éœ€è¦æ£€æŸ¥é¡µé¢: {len(needs_review)} ä¸ª\n\n"
        
        if len(successful_comparisons) == len(self.original_pages) and not needs_review:
            report += "ğŸ‰ **ç½‘ç«™ä¸‹è½½å®Œæ•´ï¼Œå†…å®¹ä¸€è‡´æ€§è‰¯å¥½ï¼**\n"
        else:
            report += "âš ï¸ **éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–å’Œä¿®å¤**\n"
        
        return report

def main():
    analyzer = WebsiteAnalyzer()
    report = analyzer.generate_report()
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = "website_comparison_report.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"\næŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
    print("\n" + "="*60)
    print(report[:1000] + "..." if len(report) > 1000 else report)

if __name__ == "__main__":
    main()