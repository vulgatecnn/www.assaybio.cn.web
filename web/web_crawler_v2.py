#!/usr/bin/env python3
"""
Web Crawler Tool v2 - ä¿æŒåŸå§‹è·¯å¾„ç»“æ„çš„ç½‘ç«™ä¸‹è½½å·¥å…·
åŠŸèƒ½ï¼šä¸‹è½½ç½‘ç«™å®Œæ•´å†…å®¹ï¼Œä¿æŒåŸå§‹ç›®å½•ç»“æ„ï¼Œä¾¿äºæœ¬åœ°æµè§ˆ
"""

import argparse
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, urlunparse
import json
import os
import time
import sys
from typing import List, Dict, Set, Optional, Tuple
import logging
import mimetypes
from pathlib import Path
import re
from concurrent.futures import ThreadPoolExecutor, as_completed


class WebCrawlerV2:
    def __init__(self, base_url: str, output_dir: str = "downloaded_site", max_depth: int = 1, 
                 delay: float = 1.0, max_workers: int = 5):
        """
        åˆå§‹åŒ–ç½‘ç»œçˆ¬è™« v2 - ä¿æŒåŸå§‹è·¯å¾„ç»“æ„
        """
        self.base_url = base_url
        self.base_domain = urlparse(base_url).netloc
        self.base_path = urlparse(base_url).path
        self.max_depth = max_depth
        self.delay = delay
        self.max_workers = max_workers
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # å­˜å‚¨ç»“æœ
        self.internal_links: Set[str] = set()
        self.external_links: Set[str] = set()
        self.downloaded_files: Dict[str, str] = {}  # URL -> local_path
        self.visited_urls: Set[str] = set()
        self.failed_downloads: Set[str] = set()
        
        # é…ç½®è¯·æ±‚å¤´
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': '*/*',
            'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive'
        }
        
        # é…ç½®æ—¥å¿—
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        self.logger = logging.getLogger(__name__)

    def is_valid_url(self, url: str) -> bool:
        """æ£€æŸ¥URLæ˜¯å¦æœ‰æ•ˆ"""
        try:
            parsed = urlparse(url)
            return bool(parsed.netloc) and bool(parsed.scheme)
        except:
            return False

    def normalize_url(self, url: str) -> str:
        """æ ‡å‡†åŒ–URL"""
        parsed = urlparse(url)
        # ç§»é™¤fragmentéƒ¨åˆ†
        normalized = urlunparse((parsed.scheme, parsed.netloc, parsed.path, 
                               parsed.params, parsed.query, ''))
        return normalized

    def is_internal_url(self, url: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå†…éƒ¨é“¾æ¥"""
        try:
            parsed = urlparse(url)
            return parsed.netloc == self.base_domain or parsed.netloc == ''
        except:
            return False

    def create_local_path(self, url: str, content_type: str = None) -> Path:
        """æ ¹æ®åŸå§‹URLè·¯å¾„åˆ›å»ºæœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼Œä¿æŒåŸå§‹ç›®å½•ç»“æ„"""
        parsed = urlparse(url)
        url_path = parsed.path.lstrip('/')
        
        # å¦‚æœURLè·¯å¾„ä¸ºç©ºæˆ–ä»¥/ç»“å°¾ï¼Œä½¿ç”¨index.html
        if not url_path or url_path.endswith('/'):
            if url_path.endswith('/'):
                local_path = self.output_dir / url_path / 'index.html'
            else:
                local_path = self.output_dir / 'index.html'
        else:
            # ä½¿ç”¨åŸå§‹è·¯å¾„
            local_path = self.output_dir / url_path
            
            # å¦‚æœæ²¡æœ‰æ‰©å±•åä¸”æ˜¯HTMLå†…å®¹ï¼Œæ·»åŠ .html
            if not local_path.suffix and content_type and 'text/html' in content_type:
                # å¯¹äºç±»ä¼¼ default.aspx, info.aspx è¿™æ ·çš„æ–‡ä»¶ï¼Œä¿æŒåŸå
                if not any(ext in url_path.lower() for ext in ['.aspx', '.php', '.jsp', '.htm']):
                    local_path = local_path.with_suffix('.html')
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        local_path.parent.mkdir(parents=True, exist_ok=True)
        
        return local_path

    def get_relative_path(self, from_path: str, to_url: str) -> str:
        """è®¡ç®—ä»ä¸€ä¸ªæ–‡ä»¶åˆ°å¦ä¸€ä¸ªURLçš„ç›¸å¯¹è·¯å¾„"""
        if not self.is_internal_url(to_url):
            return to_url
            
        to_parsed = urlparse(to_url)
        to_path = to_parsed.path.lstrip('/')
        
        # å¦‚æœç›®æ ‡URLæ˜¯æ ¹ç›®å½•æˆ–ç©ºï¼ŒæŒ‡å‘index.html
        if not to_path or to_path == '/':
            to_path = 'index.html'
        elif to_path.endswith('/'):
            to_path = to_path + 'index.html'
            
        from_dir = os.path.dirname(from_path)
        if from_dir:
            relative = os.path.relpath(to_path, from_dir)
        else:
            relative = to_path
            
        # å°†Windowsè·¯å¾„åˆ†éš”ç¬¦è½¬æ¢ä¸ºURLæ ¼å¼
        relative = relative.replace('\\', '/')
        return relative

    def download_file(self, url: str, force_download: bool = False) -> Optional[str]:
        """ä¸‹è½½æ–‡ä»¶åˆ°æœ¬åœ°ï¼Œä¿æŒåŸå§‹è·¯å¾„ç»“æ„"""
        if url in self.downloaded_files and not force_download:
            return self.downloaded_files[url]
        
        if url in self.failed_downloads:
            return None
            
        try:
            self.logger.info(f"æ­£åœ¨ä¸‹è½½: {url}")
            response = requests.get(url, headers=self.headers, timeout=30, stream=True)
            response.raise_for_status()
            
            # è·å–å†…å®¹ç±»å‹
            content_type = response.headers.get('content-type', '')
            
            # åˆ›å»ºæœ¬åœ°è·¯å¾„
            local_path = self.create_local_path(url, content_type)
            
            # ä¸‹è½½æ–‡ä»¶
            with open(local_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            
            # è®°å½•ä¸‹è½½çš„æ–‡ä»¶
            relative_path = os.path.relpath(local_path, self.output_dir)
            self.downloaded_files[url] = relative_path.replace('\\', '/')
            
            self.logger.info(f"ä¸‹è½½å®Œæˆ: {url} -> {relative_path}")
            return self.downloaded_files[url]
            
        except requests.exceptions.RequestException as e:
            self.logger.error(f"ä¸‹è½½å¤±è´¥ {url}: {e}")
            self.failed_downloads.add(url)
            return None
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥ {url}: {e}")
            self.failed_downloads.add(url)
            return None

    def get_page_content(self, url: str) -> Optional[BeautifulSoup]:
        """è·å–é¡µé¢å†…å®¹"""
        try:
            self.logger.info(f"æ­£åœ¨è·å–é¡µé¢: {url}")
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            # æ£€æŸ¥å†…å®¹ç±»å‹
            content_type = response.headers.get('content-type', '').lower()
            
            # å¦‚æœæ˜¯HTMLé¡µé¢ï¼Œè§£æå¹¶è¿”å›BeautifulSoupå¯¹è±¡
            if 'text/html' in content_type:
                soup = BeautifulSoup(response.content, 'html.parser')
                return soup
            else:
                # å¦‚æœä¸æ˜¯HTMLï¼Œç›´æ¥ä¸‹è½½æ–‡ä»¶
                if self.is_internal_url(url):
                    self.download_file(url)
                return None
                
        except requests.exceptions.RequestException as e:
            self.logger.error(f"è·å–é¡µé¢å¤±è´¥ {url}: {e}")
            return None
        except Exception as e:
            self.logger.error(f"è§£æé¡µé¢å¤±è´¥ {url}: {e}")
            return None

    def extract_all_resources(self, soup: BeautifulSoup, base_url: str) -> Dict[str, List[str]]:
        """æå–é¡µé¢ä¸­çš„æ‰€æœ‰èµ„æºé“¾æ¥"""
        resources = {
            'links': [],
            'images': [],
            'css': [],
            'js': [],
            'other': []
        }
        
        # æå–<a>æ ‡ç­¾é“¾æ¥
        for link in soup.find_all('a', href=True):
            href = link['href'].strip()
            if not href or href.startswith('#') or href.startswith('mailto:') or href.startswith('tel:'):
                continue
                
            absolute_url = urljoin(base_url, href)
            absolute_url = self.normalize_url(absolute_url)
            
            if self.is_valid_url(absolute_url):
                resources['links'].append(absolute_url)
        
        # æå–å›¾ç‰‡èµ„æº
        for img in soup.find_all('img'):
            for attr in ['src', 'data-src', 'data-original']:
                if img.get(attr):
                    src = img[attr].strip()
                    if src:
                        absolute_url = urljoin(base_url, src)
                        absolute_url = self.normalize_url(absolute_url)
                        if self.is_valid_url(absolute_url):
                            resources['images'].append(absolute_url)
                        break
        
        # æå–CSSæ–‡ä»¶
        for link in soup.find_all('link', rel='stylesheet'):
            if link.get('href'):
                href = link['href'].strip()
                absolute_url = urljoin(base_url, href)
                absolute_url = self.normalize_url(absolute_url)
                if self.is_valid_url(absolute_url):
                    resources['css'].append(absolute_url)
        
        # æå–JavaScriptæ–‡ä»¶
        for script in soup.find_all('script', src=True):
            src = script['src'].strip()
            absolute_url = urljoin(base_url, src)
            absolute_url = self.normalize_url(absolute_url)
            if self.is_valid_url(absolute_url):
                resources['js'].append(absolute_url)
        
        # æå–CSSä¸­çš„èƒŒæ™¯å›¾ç‰‡
        for element in soup.find_all(style=True):
            style = element['style']
            if 'url(' in style:
                matches = re.findall(r'url\\(["\']?(.*?)["\']?\\)', style)
                for match in matches:
                    absolute_url = urljoin(base_url, match.strip())
                    absolute_url = self.normalize_url(absolute_url)
                    if self.is_valid_url(absolute_url):
                        resources['images'].append(absolute_url)
        
        # å»é‡
        for key in resources:
            resources[key] = list(set(resources[key]))
            
        return resources

    def update_html_content(self, soup: BeautifulSoup, base_url: str, current_file_path: str) -> str:
        """æ›´æ–°HTMLå†…å®¹ï¼Œå°†å†…éƒ¨èµ„æºé“¾æ¥è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„"""
        
        # æ›´æ–°å›¾ç‰‡é“¾æ¥
        for img in soup.find_all('img'):
            for attr in ['src', 'data-src', 'data-original']:
                if img.get(attr):
                    original_url = urljoin(base_url, img[attr])
                    if self.is_internal_url(original_url):
                        relative_path = self.get_relative_path(current_file_path, original_url)
                        img[attr] = relative_path
                    break
        
        # æ›´æ–°CSSé“¾æ¥
        for link in soup.find_all('link', rel='stylesheet'):
            if link.get('href'):
                original_url = urljoin(base_url, link['href'])
                if self.is_internal_url(original_url):
                    relative_path = self.get_relative_path(current_file_path, original_url)
                    link['href'] = relative_path
        
        # æ›´æ–°JavaScripté“¾æ¥
        for script in soup.find_all('script', src=True):
            original_url = urljoin(base_url, script['src'])
            if self.is_internal_url(original_url):
                relative_path = self.get_relative_path(current_file_path, original_url)
                script['src'] = relative_path
        
        # æ›´æ–°<a>æ ‡ç­¾é“¾æ¥
        for link in soup.find_all('a', href=True):
            original_url = urljoin(base_url, link['href'])
            if self.is_internal_url(original_url):
                relative_path = self.get_relative_path(current_file_path, original_url)
                link['href'] = relative_path
        
        return str(soup)

    def crawl_page(self, url: str, depth: int = 0) -> None:
        """çˆ¬å–å•ä¸ªé¡µé¢"""
        if depth > self.max_depth or url in self.visited_urls:
            return
            
        self.visited_urls.add(url)
        
        # è·å–é¡µé¢å†…å®¹
        soup = self.get_page_content(url)
        if not soup:
            return
            
        # æå–æ‰€æœ‰èµ„æº
        resources = self.extract_all_resources(soup, url)
        
        # åˆ†ç±»å¤„ç†èµ„æº
        all_urls_to_download = []
        
        # å¤„ç†é“¾æ¥
        for link in resources['links']:
            if self.is_internal_url(link):
                self.internal_links.add(link)
            else:
                self.external_links.add(link)
        
        # æ”¶é›†æ‰€æœ‰éœ€è¦ä¸‹è½½çš„å†…éƒ¨èµ„æº
        for resource_type in ['images', 'css', 'js']:
            for resource_url in resources[resource_type]:
                if self.is_internal_url(resource_url):
                    all_urls_to_download.append(resource_url)
                else:
                    self.external_links.add(resource_url)
        
        # å¹¶å‘ä¸‹è½½æ‰€æœ‰èµ„æº
        if all_urls_to_download:
            self.logger.info(f"å¼€å§‹ä¸‹è½½ {len(all_urls_to_download)} ä¸ªèµ„æºæ–‡ä»¶")
            self.download_resources_parallel(all_urls_to_download)
        
        # åˆ›å»ºHTMLæ–‡ä»¶çš„æœ¬åœ°è·¯å¾„
        html_local_path = self.create_local_path(url, 'text/html')
        current_file_path = os.path.relpath(html_local_path, self.output_dir).replace('\\', '/')
        
        # æ›´æ–°HTMLå†…å®¹å¹¶ä¿å­˜
        updated_html = self.update_html_content(soup, url, current_file_path)
        
        try:
            with open(html_local_path, 'w', encoding='utf-8') as f:
                f.write(updated_html)
            
            self.downloaded_files[url] = current_file_path
            self.logger.info(f"HTMLé¡µé¢ä¿å­˜: {url} -> {current_file_path}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜HTMLé¡µé¢å¤±è´¥ {url}: {e}")
        
        self.logger.info(f"é¡µé¢ {url} - å‘ç° {len(resources['links'])} ä¸ªé“¾æ¥, "
                        f"{len(all_urls_to_download)} ä¸ªå†…éƒ¨èµ„æº")
        
        # å¦‚æœéœ€è¦æ·±åº¦çˆ¬å–ï¼Œç»§ç»­çˆ¬å–åŒåŸŸåä¸‹çš„HTMLé¡µé¢
        if depth < self.max_depth:
            internal_html_links = [link for link in resources['links'] 
                                 if self.is_internal_url(link)
                                 and link not in self.visited_urls]
            
            for link in internal_html_links[:10]:  # é™åˆ¶æ¯é¡µæœ€å¤šçˆ¬å–10ä¸ªå­é¡µé¢
                time.sleep(self.delay)
                self.crawl_page(link, depth + 1)
    
    def download_resources_parallel(self, urls: List[str]) -> None:
        """å¹¶å‘ä¸‹è½½èµ„æº"""
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_url = {executor.submit(self.download_file, url): url for url in urls}
            
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    result = future.result()
                    if result:
                        self.logger.debug(f"èµ„æºä¸‹è½½å®Œæˆ: {url}")
                except Exception as e:
                    self.logger.error(f"å¹¶å‘ä¸‹è½½å¤±è´¥ {url}: {e}")

    def crawl(self) -> Dict:
        """å¼€å§‹çˆ¬å–"""
        self.logger.info(f"å¼€å§‹ä¸‹è½½ç½‘ç«™: {self.base_url}")
        self.logger.info(f"æœ€å¤§æ·±åº¦: {self.max_depth}, å»¶æ—¶: {self.delay}ç§’")
        self.logger.info(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        
        start_time = time.time()
        
        try:
            self.crawl_page(self.base_url)
        except KeyboardInterrupt:
            self.logger.info("ç”¨æˆ·ä¸­æ–­ä¸‹è½½...")
        
        end_time = time.time()
        
        result = {
            'base_url': self.base_url,
            'output_directory': str(self.output_dir),
            'crawl_time': end_time - start_time,
            'total_pages_visited': len(self.visited_urls),
            'total_internal_links': len(self.internal_links),
            'total_external_links': len(self.external_links),
            'total_downloaded_files': len(self.downloaded_files),
            'total_failed_downloads': len(self.failed_downloads),
            'internal_links': sorted(list(self.internal_links)),
            'external_links': sorted(list(self.external_links)),
            'downloaded_files': dict(self.downloaded_files),
            'failed_downloads': sorted(list(self.failed_downloads)),
            'visited_pages': sorted(list(self.visited_urls))
        }
        
        self.logger.info(f"ä¸‹è½½å®Œæˆ! ç”¨æ—¶: {result['crawl_time']:.2f}ç§’")
        self.logger.info(f"è®¿é—®é¡µé¢: {result['total_pages_visited']} ä¸ª")
        self.logger.info(f"å†…éƒ¨é“¾æ¥: {result['total_internal_links']} ä¸ª")
        self.logger.info(f"å¤–éƒ¨é“¾æ¥: {result['total_external_links']} ä¸ª")
        self.logger.info(f"ä¸‹è½½æ–‡ä»¶: {result['total_downloaded_files']} ä¸ª")
        self.logger.info(f"ä¸‹è½½å¤±è´¥: {result['total_failed_downloads']} ä¸ª")
        
        # åˆ›å»ºç´¢å¼•é¡µé¢
        self.create_index_page(result)
        
        return result
    
    def create_index_page(self, result: Dict) -> None:
        """åˆ›å»ºç´¢å¼•é¡µé¢"""
        try:
            index_html = f"""<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç½‘ç«™ä¸‹è½½å®Œæˆ - {self.base_domain}</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }}
        h1, h2 {{ color: #333; }}
        .stats {{ background: #f4f4f4; padding: 20px; border-radius: 5px; margin: 20px 0; }}
        .file-list {{ max-height: 300px; overflow-y: auto; border: 1px solid #ddd; padding: 10px; }}
        .external-link {{ color: #e74c3c; }}
        .internal-link {{ color: #27ae60; }}
        .main-link {{ font-size: 18px; font-weight: bold; color: #2980b9; }}
    </style>
</head>
<body>
    <h1>ç½‘ç«™ä¸‹è½½å®Œæˆ</h1>
    <div class="stats">
        <h2>ä¸‹è½½ç»Ÿè®¡</h2>
        <p><strong>åŸå§‹ç½‘ç«™:</strong> <a href="{result['base_url']}" target="_blank">{result['base_url']}</a></p>
        <p><strong>ä¸‹è½½æ—¶é—´:</strong> {result['crawl_time']:.2f} ç§’</p>
        <p><strong>è®¿é—®é¡µé¢:</strong> {result['total_pages_visited']} ä¸ª</p>
        <p><strong>ä¸‹è½½æ–‡ä»¶:</strong> {result['total_downloaded_files']} ä¸ª</p>
        <p><strong>å†…éƒ¨é“¾æ¥:</strong> {result['total_internal_links']} ä¸ª</p>
        <p><strong>å¤–éƒ¨é“¾æ¥:</strong> {result['total_external_links']} ä¸ª</p>
        <p><strong>ä¸‹è½½å¤±è´¥:</strong> {result['total_failed_downloads']} ä¸ª</p>
    </div>

    <h2>ä¸»é¡µé¢</h2>
    <p><a href="index.html" class="main-link">ç‚¹å‡»è¿™é‡Œè®¿é—®ä¸‹è½½çš„ç½‘ç«™ä¸»é¡µ</a></p>

    <h2>å·²ä¸‹è½½çš„é¡µé¢</h2>
    <div class="file-list">
        <ul>
"""
            
            # æ·»åŠ HTMLé¡µé¢é“¾æ¥
            html_files = [(url, path) for url, path in result['downloaded_files'].items() 
                         if path.endswith('.html') or '.aspx' in path]
            
            for original_url, local_path in sorted(html_files):
                index_html += f'<li><a href="{local_path}" class="internal-link">{original_url}</a></li>\\n'
            
            index_html += f"""
        </ul>
    </div>

    <h2>å¤–éƒ¨é“¾æ¥ (æœªä¸‹è½½)</h2>
    <div class="file-list">
        <ul>
"""
            
            for external_link in sorted(result['external_links'])[:20]:  # æœ€å¤šæ˜¾ç¤º20ä¸ªå¤–éƒ¨é“¾æ¥
                index_html += f'<li><a href="{external_link}" target="_blank" class="external-link">{external_link}</a></li>\\n'
            
            index_html += """
        </ul>
    </div>

    <h2>ä½¿ç”¨è¯´æ˜</h2>
    <p>ç½‘ç«™å·²æŒ‰åŸå§‹ç›®å½•ç»“æ„ä¿å­˜ï¼Œå¯ä»¥ç›´æ¥ç”¨æµè§ˆå™¨æ‰“å¼€ä¸»é¡µé¢æµè§ˆã€‚</p>
    <p>æ‰€æœ‰å†…éƒ¨é“¾æ¥å’Œèµ„æºæ–‡ä»¶å·²è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„ï¼Œç¡®ä¿æœ¬åœ°æ­£å¸¸è®¿é—®ã€‚</p>
</body>
</html>
"""
            
            index_path = self.output_dir / 'site_index.html'
            with open(index_path, 'w', encoding='utf-8') as f:
                f.write(index_html)
                
            self.logger.info(f"ç´¢å¼•é¡µé¢å·²åˆ›å»º: {index_path}")
            
        except Exception as e:
            self.logger.error(f"åˆ›å»ºç´¢å¼•é¡µé¢å¤±è´¥: {e}")


def main():
    parser = argparse.ArgumentParser(description='Web Crawler v2 - ä¿æŒåŸå§‹è·¯å¾„ç»“æ„çš„ç½‘ç«™ä¸‹è½½å·¥å…·')
    parser.add_argument('url', help='è¦ä¸‹è½½çš„ç½‘ç«™URL')
    parser.add_argument('-d', '--depth', type=int, default=1, help='çˆ¬å–æ·±åº¦ (é»˜è®¤: 1)')
    parser.add_argument('-o', '--output-dir', default='downloaded_site_v2', help='è¾“å‡ºç›®å½• (é»˜è®¤: downloaded_site_v2)')
    parser.add_argument('--delay', type=float, default=1.0, help='è¯·æ±‚é—´éš”æ—¶é—´(ç§’) (é»˜è®¤: 1.0)')
    parser.add_argument('--max-workers', type=int, default=5, help='å¹¶å‘ä¸‹è½½çº¿ç¨‹æ•° (é»˜è®¤: 5)')
    parser.add_argument('-v', '--verbose', action='store_true', help='è¯¦ç»†è¾“å‡º')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # éªŒè¯URLæ ¼å¼
    if not args.url.startswith(('http://', 'https://')):
        args.url = 'http://' + args.url
    
    try:
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        crawler = WebCrawlerV2(
            base_url=args.url, 
            output_dir=args.output_dir,
            max_depth=args.depth, 
            delay=args.delay,
            max_workers=args.max_workers
        )
        
        print(f"å¼€å§‹ä¸‹è½½ç½‘ç«™: {args.url}")
        print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
        print(f"ä¿æŒåŸå§‹è·¯å¾„ç»“æ„ï¼Œä¾¿äºæœ¬åœ°æµè§ˆ")
        print("-" * 50)
        
        # å¼€å§‹ä¸‹è½½
        result = crawler.crawl()
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = "download_report_v2.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°æ¦‚è¦ä¿¡æ¯
        print("\\n" + "="*60)
        print("ä¸‹è½½å®Œæˆ!")
        print("="*60)
        print(f"åŸå§‹ç½‘ç«™: {result['base_url']}")
        print(f"è¾“å‡ºç›®å½•: {result['output_directory']}")
        print(f"ä¸‹è½½è€—æ—¶: {result['crawl_time']:.2f}ç§’")
        print(f"è®¿é—®é¡µé¢: {result['total_pages_visited']} ä¸ª")
        print(f"ä¸‹è½½æ–‡ä»¶: {result['total_downloaded_files']} ä¸ª")
        
        main_index = os.path.join(args.output_dir, 'index.html')
        if os.path.exists(main_index):
            print(f"\\nğŸŒ æ‰“å¼€ä¸»é¡µé¢: {main_index}")
        print(f"ğŸ“‹ æŸ¥çœ‹ä¸‹è½½ç´¢å¼•: {os.path.join(args.output_dir, 'site_index.html')}")
        
    except KeyboardInterrupt:
        print("\\nç”¨æˆ·ä¸­æ–­ä¸‹è½½")
        sys.exit(1)
    except Exception as e:
        print(f"ç¨‹åºæ‰§è¡Œé”™è¯¯: {e}")
        logging.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        sys.exit(1)


if __name__ == '__main__':
    main()